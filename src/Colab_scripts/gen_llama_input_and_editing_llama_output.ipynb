{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22786,
     "status": "ok",
     "timestamp": 1736426828367,
     "user": {
      "displayName": "Meriç Uluçay",
      "userId": "04294619280493082833"
     },
     "user_tz": -60
    },
    "id": "kvpEanyU3-mw",
    "outputId": "566b6536-a8fd-4daf-ad91-f83130764880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTSvhBsBvnXy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = \"AKIATEEVKTGZPSUYBPTP\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12117,
     "status": "ok",
     "timestamp": 1736426968274,
     "user": {
      "displayName": "Meriç Uluçay",
      "userId": "04294619280493082833"
     },
     "user_tz": -60
    },
    "id": "D-D9Jm-l162m",
    "outputId": "1ca45135-fe02-4970-e4d8-b2a5cb16f997"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 64.4M  100 64.4M    0     0   127M      0 --:--:-- --:--:-- --:--:--  127M\n"
     ]
    }
   ],
   "source": [
    "# Download the AWS and Ego4D CLIs, then download the annotations locally\n",
    "%%bash\n",
    "\n",
    "# Set up the AWS CLI\n",
    "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "unzip -o awscliv2.zip >/dev/null\n",
    "sudo ./aws/install >/dev/null 2>&1\n",
    "aws configure set aws_access_key_id \"$AWS_ACCESS_KEY_ID\" && aws configure set aws_secret_access_key \"$AWS_SECRET_ACCESS_KEY\"\n",
    "rm \"awscliv2.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20029,
     "status": "ok",
     "timestamp": 1736426996674,
     "user": {
      "displayName": "Meriç Uluçay",
      "userId": "04294619280493082833"
     },
     "user_tz": -60
    },
    "id": "Jg6Xt1p-On-a",
    "outputId": "40d06083-eeb3-4840-f729-10b5d647dc43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ego4d\n",
      "  Downloading ego4d-1.7.3.tar.gz (94 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/94.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m92.2/94.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting boto3 (from ego4d)\n",
      "  Downloading boto3-1.35.95-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ego4d) (4.67.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from ego4d) (2024.11.6)\n",
      "Collecting dataclasses_json (from ego4d)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting iopath (from ego4d)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting botocore<1.36.0,>=1.35.95 (from boto3->ego4d)\n",
      "  Downloading botocore-1.35.95-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->ego4d)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->ego4d)\n",
      "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses_json->ego4d)\n",
      "  Downloading marshmallow-3.24.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses_json->ego4d)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath->ego4d) (4.12.2)\n",
      "Collecting portalocker (from iopath->ego4d)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.95->boto3->ego4d) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.95->boto3->ego4d) (2.3.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses_json->ego4d) (24.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses_json->ego4d)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.95->boto3->ego4d) (1.17.0)\n",
      "Downloading boto3-1.35.95-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading botocore-1.35.95-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading marshmallow-3.24.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Building wheels for collected packages: ego4d, iopath\n",
      "  Building wheel for ego4d (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ego4d: filename=ego4d-1.7.3-py3-none-any.whl size=118282 sha256=1a542158e06dab4e1ed5c4664a1d14731a7771bfda0f56b5da36592882c27b4c\n",
      "  Stored in directory: /root/.cache/pip/wheels/65/a8/89/a6187e3bc9a85e81899ab8d5ddc2011c9954d3b6cb84d47e03\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=964b51f59c02fca5ec9969d909c1fd1199db813f683ec600a7fdc00f04027fca\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "Successfully built ego4d iopath\n",
      "Installing collected packages: portalocker, mypy-extensions, marshmallow, jmespath, typing-inspect, iopath, botocore, s3transfer, dataclasses_json, boto3, ego4d\n",
      "Successfully installed boto3-1.35.95 botocore-1.35.95 dataclasses_json-0.6.7 ego4d-1.7.3 iopath-0.1.10 jmespath-1.0.1 marshmallow-3.24.2 mypy-extensions-1.0.0 portalocker-3.1.1 s3transfer-0.10.4 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# Set up the Ego4D CLI\n",
    "!pip install ego4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270972,
     "status": "ok",
     "timestamp": 1736427272282,
     "user": {
      "displayName": "Meriç Uluçay",
      "userId": "04294619280493082833"
     },
     "user_tz": -60
    },
    "id": "tcKr9i88KMaa",
    "outputId": "9f104671-a677-44d2-c922-928b77ae9639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Non-standard Dataset Specfied (Allowed, will attempt download): ['omnivore_video_swinl_fp16']\n",
      "Datasets to download: {'omnivore_video_swinl_fp16', 'annotations'}\n",
      "Download Path: /content/ego4d_data/v1\n",
      "Downloading Ego4D metadata json..\n",
      "Ego4D Metadata: /content/ego4d_data/ego4d.json\n",
      "Checking requested datasets and versions...\n",
      "Created download directory for version 'v1' of dataset: 'omnivore_video_swinl_fp16' at: /content/ego4d_data/v1/omnivore_video_swinl_fp16\n",
      "Filtering by benchmarks: ['nlq']\n",
      "Created download directory for version 'v1' of dataset: 'annotations' at: /content/ego4d_data/v1/annotations\n",
      "Benchmarks specified but ignored without a benchmarks field in manifest.\n",
      "Retrieving object metadata from S3...\n",
      "100% 1290/1290 [00:02<00:00, 471.49object/s]\n",
      "Checking if latest file versions are already downloaded...\n",
      " 97% 1250/1290 [00:14<00:00, 187.45file/s]WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-utokyo.s3.us-west-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-consortium-sharing.s3.us-west-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-consortium-sharing.s3.us-west-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-consortium-sharing.s3.us-west-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-consortium-sharing.s3.us-west-1.amazonaws.com. Connection pool size: 10\n",
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: ego4d-consortium-sharing.s3.us-west-1.amazonaws.com. Connection pool size: 10\n",
      "100% 1290/1290 [00:15<00:00, 85.65file/s] \n",
      "No existing videos to filter.\n",
      "Downloading 1290 files..\n",
      "100% 12.8G/12.8G [04:05<00:00, 106MiB/s]Checking file integrity...\n",
      "100% 12.8G/12.8G [04:05<00:00, 56.1MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the Ego4D Annotations to ego4d_data/\n",
    "!ego4d --output_directory=\"/content/ego4d_data/\" --datasets annotations omnivore_video_swinl_fp16 --benchmarks nlq -y --version v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ml-o3VcisBri"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_name = \"/content/ego4d_data/v1/omnivore_video_swinl_fp16\"\n",
    "\n",
    "docs = os.listdir(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdbOlIoNsP30"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store processed file names\n",
    "omnivore = []\n",
    "\n",
    "# Iterate over each item in the list `docs`\n",
    "for i in docs:\n",
    "    # Extract the file name without its extension by splitting at the '.' character\n",
    "    # and taking the part before the first occurrence of '.'\n",
    "    omnivore.append(i.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRC53c7a78Cu"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/content/ego4d_data/v1/annotations/nlq_val.json', 'r') as file:\n",
    "    val = json.load(file)\n",
    "with open('/content/ego4d_data/v1/annotations/nlq_train.json', 'r') as file:\n",
    "    train = json.load(file)\n",
    "with open('/content/ego4d_data/v1/annotations/nlq_test_unannotated.json', 'r') as file:\n",
    "    test = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1736427762853,
     "user": {
      "displayName": "Meriç Uluçay",
      "userId": "04294619280493082833"
     },
     "user_tz": -60
    },
    "id": "leqXnpCG8c99",
    "outputId": "7d9273c0-758b-4fdd-9b36-829a34ff5e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1659\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store all clip IDs from train, val, and test datasets\n",
    "nlq_clip_list = []\n",
    "\n",
    "# Iterate through all videos in the train dataset\n",
    "for i in train[\"videos\"]:\n",
    "    # Iterate through all clips within the current video\n",
    "    for j in i[\"clips\"]:\n",
    "        # Append the clip_uid of the current clip to the list\n",
    "        nlq_clip_list.append(j[\"clip_uid\"])\n",
    "\n",
    "# Repeat the same process for the validation dataset\n",
    "for i in val[\"videos\"]:\n",
    "    for j in i[\"clips\"]:\n",
    "        nlq_clip_list.append(j[\"clip_uid\"])\n",
    "\n",
    "# Repeat the same process for the test dataset\n",
    "for i in test[\"videos\"]:\n",
    "    for j in i[\"clips\"]:\n",
    "        nlq_clip_list.append(j[\"clip_uid\"])\n",
    "\n",
    "# Print the total number of unique clip IDs collected across all datasets\n",
    "print(len(nlq_clip_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPDx-WOBSKrq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/content/drive/MyDrive/VSLNet/ego4d.json', 'r') as file:\n",
    "    ego4d = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sxh3_i_SfdrA"
   },
   "outputs": [],
   "source": [
    "# Flag to indicate whether a new video needs to be added\n",
    "new_video = True\n",
    "\n",
    "# Dictionary to store available clips grouped by videos\n",
    "avalible_clips = {\"videos\": []}\n",
    "\n",
    "# Iterate through all clips in the ego4d dataset\n",
    "for clip in ego4d[\"clips\"]:\n",
    "    # Check if the current clip's video UID exists in the omnivore list\n",
    "    if clip[\"video_uid\"] in omnivore:\n",
    "        # Check if the clip UID is not already in the nlq_clip_list\n",
    "        if clip[\"clip_uid\"] not in nlq_clip_list:\n",
    "            # Create a dictionary for the new clip with its start and end times\n",
    "            new_clip = {\n",
    "                \"clip_uid\": clip[\"clip_uid\"],\n",
    "                \"video_start_sec\": clip[\"video_start_sec\"],\n",
    "                \"video_end_sec\": clip[\"video_end_sec\"],\n",
    "            }\n",
    "\n",
    "            # Check if the video already exists in avalible_clips\n",
    "            for video in avalible_clips[\"videos\"]:\n",
    "                if video[\"video_uid\"] == clip[\"video_uid\"]:\n",
    "                    # If the video exists, append the new clip to its clips list\n",
    "                    video[\"clips\"].append(new_clip)\n",
    "                    # Set the flag to False since this is not a new video\n",
    "                    new_video = False\n",
    "                    break\n",
    "\n",
    "            # If the video is not in avalible_clips, add it as a new entry\n",
    "            if new_video:\n",
    "                video_dict = {\n",
    "                    \"video_uid\": clip[\"video_uid\"],  # The UID of the video\n",
    "                    \"clips\": [new_clip],  # Start with the new clip\n",
    "                }\n",
    "                # Append the new video with its clip to the avalible_clips list\n",
    "                avalible_clips[\"videos\"].append(video_dict)\n",
    "\n",
    "            # Reset the new_video flag for the next iteration\n",
    "            new_video = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PVCE5fnpEWw"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load file\n",
    "with open(\"/content/drive/MyDrive/VSLNet/v1/annotations/narration.json\", \"r\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAI3PZZAojT7"
   },
   "outputs": [],
   "source": [
    "import os  # Provides functions for interacting with the operating system\n",
    "\n",
    "a = 0  # Counter for tracking the number of processed segments (optional)\n",
    "output_dir = \"/content/drive/MyDrive/VSLNet/new_narrations\"  # Directory to save the output JSON files\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't already exist\n",
    "\n",
    "# Iterate over all videos in the available clips dictionary\n",
    "for video_info in avalible_clips[\"videos\"]:\n",
    "    available_video_uid = video_info[\"video_uid\"]  # Extract the video UID\n",
    "\n",
    "    # Skip processing if the video UID is not found in the main data dictionary\n",
    "    if available_video_uid not in data:\n",
    "        # a += 1  # (Optional) Uncomment if you want to track skipped videos\n",
    "        continue\n",
    "\n",
    "    narration_passes = data[available_video_uid]  # Get all narration passes for this video UID\n",
    "\n",
    "    # Iterate over all clips for the current video\n",
    "    for clip_info in video_info[\"clips\"]:\n",
    "        clip_uid = clip_info[\"clip_uid\"]  # Unique identifier for the current clip\n",
    "        clip_start = clip_info[\"video_start_sec\"]  # Start time of the clip in seconds\n",
    "        clip_end = clip_info[\"video_end_sec\"]  # End time of the clip in seconds\n",
    "\n",
    "        filtered_narrations = []  # List to store narrations within the clip's time range\n",
    "\n",
    "        # Iterate over narration passes and their associated narrations\n",
    "        for narration_pass, narration_data in narration_passes.items():\n",
    "            if type(narration_data) == dict:  # Ensure the narration data is a dictionary\n",
    "                # Filter narrations that fall within the clip's time range\n",
    "                for narration in narration_data[\"narrations\"]:\n",
    "                    if clip_start <= narration[\"timestamp_sec\"] <= clip_end:\n",
    "                        filtered_narrations.append(narration)\n",
    "\n",
    "        segments = {}  # Dictionary to group narrations into 150-second segments\n",
    "\n",
    "        # Assign each narration to its respective segment based on its timestamp\n",
    "        for narration in filtered_narrations:\n",
    "            timestamp = narration[\"timestamp_sec\"]  # Get the narration's timestamp\n",
    "            segment = int((timestamp - clip_start) // 150)  # Calculate the segment number\n",
    "            if segment not in segments:\n",
    "                segments[segment] = []  # Create a new segment if it doesn't exist\n",
    "            segments[segment].append(narration)  # Add the narration to the segment\n",
    "\n",
    "        # Save each segment to a separate JSON file\n",
    "        for segment, segment_narrations in segments.items():\n",
    "            a += 1  # Increment the segment counter\n",
    "            output_file_name = f\"{available_video_uid}_{clip_uid}_segment_{segment}.json\"  # Define the file name\n",
    "            output_file_path = os.path.join(output_dir, output_file_name)  # Full path for the output file\n",
    "\n",
    "            # Prepare the segment data for saving\n",
    "            segment_data = {\"narrations\": segment_narrations}\n",
    "\n",
    "            # Write the segment data to the JSON file\n",
    "            with open(output_file_path, \"w\") as outfile:\n",
    "                json.dump(segment_data, outfile, indent=4)  # Save the data with indentation for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtfgkCNQ5oKb"
   },
   "source": [
    "Editing Llama Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6rkwlyepBde"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "tar_file_path = \"/content/run_1_output.tar.gz\"\n",
    "\n",
    "extract_to = \"/content/new_queries_1\"\n",
    "\n",
    "with tarfile.open(tar_file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeEmrQ0C3ShL"
   },
   "outputs": [],
   "source": [
    "from threading import excepthook  # Module for handling exceptions in threading (not explicitly used here)\n",
    "import os  # For interacting with the file system\n",
    "import json  # For parsing and writing JSON data\n",
    "\n",
    "num = 0  # Counter to track the total number of processed queries\n",
    "\n",
    "# Path to the directory containing input JSON files\n",
    "input_dir = \"/content/new_queries_1/run_1_output\"\n",
    "\n",
    "# Extract existing video_uids and clip_uids from the training data\n",
    "added_videos = [video[\"video_uid\"] for video in train[\"videos\"]]  # List of video_uids already in the dataset\n",
    "added_clips = [clip[\"clip_uid\"] for video in train[\"videos\"] for clip in video[\"clips\"]]  # List of clip_uids already in the dataset\n",
    "\n",
    "# Initialize the new dataset using the existing training data\n",
    "new_nlq_train = train\n",
    "\n",
    "# Iterate through all files in the input directory, sorted alphabetically\n",
    "for file_name in sorted(os.listdir(input_dir)):\n",
    "    file_path = os.path.join(input_dir, file_name)  # Get the full path of the file\n",
    "\n",
    "    # Ensure the path corresponds to a file\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            try:\n",
    "                data = json.load(file)  # Load the JSON content from the file\n",
    "                video_uid = file_name.split(\"_\")[0]  # Extract video_uid from the file name\n",
    "                clip_uid = file_name.split(\"_\")[1]  # Extract clip_uid from the file name\n",
    "\n",
    "                # If the video is not already in the dataset\n",
    "                if video_uid not in added_videos:\n",
    "                    video = {\"video_uid\": video_uid, \"clips\": []}  # Initialize the video structure\n",
    "                    clip = {\"clip_uid\": clip_uid}  # Initialize the clip structure\n",
    "\n",
    "                    # Find the metadata for the clip in available clips\n",
    "                    for v in avalible_clips[\"videos\"]:\n",
    "                        if v[\"video_uid\"] == video_uid:  # Match video_uid\n",
    "                            for c in v[\"clips\"]:\n",
    "                                if c[\"clip_uid\"] == clip_uid:  # Match clip_uid\n",
    "                                    # Populate time-related metadata for the clip\n",
    "                                    clip[\"video_start_sec\"] = c[\"video_start_sec\"]\n",
    "                                    clip[\"video_end_sec\"] = c[\"video_end_sec\"]\n",
    "                                    clip[\"clip_start_sec\"] = 0\n",
    "                                    clip[\"clip_end_sec\"] = c[\"video_end_sec\"] - c[\"video_start_sec\"]\n",
    "\n",
    "                    annotations = [data]  # Initialize annotations for the clip\n",
    "                    annotations[0][\"annotation_uid\"] = \"ca7e11a2-0000-0000-0000-ea810ab6a99b\"  # Set a unique annotation UID\n",
    "\n",
    "                    # Process each query in the annotations\n",
    "                    for query_index in range(len(annotations[0][\"language_queries\"])):\n",
    "                        try:\n",
    "                            # Extract the query text from the current annotation\n",
    "                            query = annotations[0][\"language_queries\"][query_index][\"query\"]\n",
    "\n",
    "                            # Replace unwanted patterns in the query text\n",
    "                            if \" C C \" in query or \" #C C \" in query:\n",
    "                                annotations[0][\"language_queries\"][query_index][\"query\"] = query.replace(\" C C \", \" C \").replace(\" #C C \", \" C \")\n",
    "\n",
    "                            # Calculate the clip-relative start and end times for the query\n",
    "                            annotations[0][\"language_queries\"][query_index][\"clip_start_sec\"] = (annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] - clip[\"video_start_sec\"])\n",
    "                            annotations[0][\"language_queries\"][query_index][\"clip_end_sec\"] = (annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - clip[\"video_start_sec\"])\n",
    "\n",
    "                            # Filter out invalid queries based on start and end times\n",
    "                            if annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] < annotations[0][\"language_queries\"][query_index][\"video_start_sec\"]:\n",
    "                                # Remove queries where the end time is earlier than the start time\n",
    "                                annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                num -= 1\n",
    "\n",
    "                            elif annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < 1:\n",
    "                                # Adjust queries shorter than 1 second\n",
    "                                annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] += 1\n",
    "                                annotations[0][\"language_queries\"][query_index][\"clip_end_sec\"] += 1\n",
    "\n",
    "                                # Check if the adjusted query still falls outside the clip boundaries\n",
    "                                if (\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < clip[\"video_start_sec\"] or\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] > clip[\"video_end_sec\"]\n",
    "                                ):\n",
    "                                    annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                    num -= 1\n",
    "\n",
    "                            elif annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] > 8:\n",
    "                                # Remove queries longer than 8 seconds\n",
    "                                annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                num -= 1\n",
    "\n",
    "                            else:\n",
    "                                # Final check: Ensure the query fits within the clip boundaries\n",
    "                                if (\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < clip[\"video_start_sec\"] or\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] > clip[\"video_end_sec\"]\n",
    "                                ):\n",
    "                                    annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                    num -= 1\n",
    "                        except:\n",
    "                            # Remove problematic queries and decrement counter\n",
    "                            annotations[0][\"language_queries\"].pop(query_index)\n",
    "                            num -= 1\n",
    "\n",
    "                    clip[\"annotations\"] = annotations  # Add annotations to the clip\n",
    "                    num += 10  # Increment the query counter\n",
    "                    video[\"clips\"].append(clip)  # Add the clip to the video structure\n",
    "                    new_nlq_train[\"videos\"].append(video)  # Add the video to the dataset\n",
    "                    added_videos.append(video_uid)  # Mark the video as processed\n",
    "                    added_clips.append(clip_uid)  # Mark the clip as processed\n",
    "\n",
    "                    # Stop processing if limit exceeded\n",
    "                    if num > 9000:\n",
    "                        break\n",
    "                else:\n",
    "                    if clip_uid not in added_clips:\n",
    "                    #if the video is included and clip is not included\n",
    "                        clip = {\"clip_uid\": clip_uid}  # Initialize the clip structure\n",
    "\n",
    "                        # Find the metadata for the clip in available clips\n",
    "                        for v in avalible_clips[\"videos\"]:\n",
    "                            if v[\"video_uid\"] == video_uid:  # Match video_uid\n",
    "                                for c in v[\"clips\"]:\n",
    "                                    if c[\"clip_uid\"] == clip_uid:  # Match clip_uid\n",
    "                                        # Populate time-related metadata for the clip\n",
    "                                        clip[\"video_start_sec\"] = c[\"video_start_sec\"]\n",
    "                                        clip[\"video_end_sec\"] = c[\"video_end_sec\"]\n",
    "                                        clip[\"clip_start_sec\"] = 0\n",
    "                                        clip[\"clip_end_sec\"] = c[\"video_end_sec\"] - c[\"video_start_sec\"]\n",
    "\n",
    "                        annotations = [data]  # Initialize annotations for the clip\n",
    "                        annotations[0][\"annotation_uid\"] = \"ca7e11a2-0000-0000-0000-ea810ab6a99b\"  # Set a unique annotation UID\n",
    "\n",
    "                        # Process each query in the annotations\n",
    "                        for query_index in range(len(annotations[0][\"language_queries\"])):\n",
    "                            try:\n",
    "                                # Extract the query text from the current annotation\n",
    "                                query = annotations[0][\"language_queries\"][query_index][\"query\"]\n",
    "\n",
    "                                # Replace unwanted patterns in the query text\n",
    "                                if \" C C \" in query or \" #C C \" in query:\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"query\"] = query.replace(\" C C \", \" C \").replace(\" #C C \", \" C \")\n",
    "\n",
    "                                # Calculate the clip-relative start and end times for the query\n",
    "                                annotations[0][\"language_queries\"][query_index][\"clip_start_sec\"] = (annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] - clip[\"video_start_sec\"])\n",
    "                                annotations[0][\"language_queries\"][query_index][\"clip_end_sec\"] = (annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - clip[\"video_start_sec\"])\n",
    "\n",
    "                                # Filter out invalid queries based on start and end times\n",
    "                                if annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] < annotations[0][\"language_queries\"][query_index][\"video_start_sec\"]:\n",
    "                                    # Remove queries where the end time is earlier than the start time\n",
    "                                    annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                    num -= 1\n",
    "\n",
    "                                elif annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < 1:\n",
    "                                    # Adjust queries shorter than 1 second\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] += 1\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"clip_end_sec\"] += 1\n",
    "\n",
    "                                    # Check if the adjusted query still falls outside the clip boundaries\n",
    "                                    if (\n",
    "                                        annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < clip[\"video_start_sec\"] or\n",
    "                                        annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] > clip[\"video_end_sec\"]\n",
    "                                    ):\n",
    "                                        annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                        num -= 1\n",
    "\n",
    "                                elif annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] > 8:\n",
    "                                    # Remove queries longer than 8 seconds\n",
    "                                    annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                    num -= 1\n",
    "\n",
    "                                else:\n",
    "                                    # Final check: Ensure the query fits within the clip boundaries\n",
    "                                    if (\n",
    "                                        annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < clip[\"video_start_sec\"] or\n",
    "                                        annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] > clip[\"video_end_sec\"]\n",
    "                                    ):\n",
    "                                        annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                        num -= 1\n",
    "                            except:\n",
    "                                # Remove problematic queries and decrement counter\n",
    "                                annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                num -= 1\n",
    "                        clip[\"annotations\"]=annotations # Add annotations to the clip\n",
    "                        for video in new_nlq_train[\"videos\"]:\n",
    "                            #print(video)\n",
    "                            if video[\"video_uid\"]==video_uid:\n",
    "                                video[\"clips\"].append(clip) # Add the clip to the video structure\n",
    "                        added_clips.append(clip_uid)  # Mark the clip as processed\n",
    "                        num+=10 # Increment the query counter\n",
    "                        # Stop processing if limit exceeded\n",
    "                        if num>9000:\n",
    "                            break\n",
    "                    else:\n",
    "                      ##if the video is included and clip is included\n",
    "                      #print(\"hi\")\n",
    "                      # Iterate through all videos in the dataset\n",
    "                      for video in new_nlq_train[\"videos\"]:\n",
    "                          # Check if the current video matches the video_uid being processed\n",
    "                          if video[\"video_uid\"] == video_uid:\n",
    "                              # Iterate through all clips in the matching video\n",
    "                              for clip in video[\"clips\"]:\n",
    "                                  # Check if the current clip matches the clip_uid being processed\n",
    "                                  if clip[\"clip_uid\"] == clip_uid:\n",
    "                                      # Iterate through all queries in the input data\n",
    "                                      for query_index in range(len(data[\"language_queries\"])):\n",
    "                                          try:\n",
    "                                              # Extract the query text for the current annotation\n",
    "                                              query = data[\"language_queries\"][query_index][\"query\"]\n",
    "\n",
    "                                              # Replace unwanted patterns in the query text\n",
    "                                              if \" C C \" in query or \" #C C \" in query:\n",
    "                                                  data[\"language_queries\"][query_index][\"query\"] = query.replace(\" C C \", \" C \").replace(\" #C C \", \" C \")\n",
    "\n",
    "                                              # Calculate clip-relative start and end times for the query\n",
    "                                              data[\"language_queries\"][query_index][\"clip_start_sec\"] = (data[\"language_queries\"][query_index][\"video_start_sec\"] - clip[\"video_start_sec\"])\n",
    "                                              data[\"language_queries\"][query_index][\"clip_end_sec\"] = (data[\"language_queries\"][query_index][\"video_end_sec\"] - clip[\"video_start_sec\"])\n",
    "\n",
    "                                              # Validate query timing: Skip if the end time is earlier than the start time\n",
    "                                              if data[\"language_queries\"][query_index][\"video_end_sec\"] < data[\"language_queries\"][query_index][\"video_start_sec\"]:\n",
    "                                                  continue\n",
    "\n",
    "                                              # Adjust queries shorter than 1 second\n",
    "                                              if data[\"language_queries\"][query_index][\"video_end_sec\"] - data[\"language_queries\"][query_index][\"video_start_sec\"] < 1:\n",
    "                                                  data[\"language_queries\"][query_index][\"video_end_sec\"] += 1  # Extend the query end time by 1 second\n",
    "                                                  data[\"language_queries\"][query_index][\"clip_end_sec\"] += 1  # Update the clip-relative end time accordingly\n",
    "\n",
    "                                              # Skip queries longer than 8 seconds\n",
    "                                              if annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] > 8:\n",
    "                                                  continue\n",
    "\n",
    "                                              # Ensure the query's start and end times fit within the clip's boundaries\n",
    "                                              if data[\"language_queries\"][query_index][\"video_start_sec\"] < clip[\"video_start_sec\"]:\n",
    "                                                  continue\n",
    "                                              if data[\"language_queries\"][query_index][\"video_end_sec\"] > clip[\"video_end_sec\"]:\n",
    "                                                  continue\n",
    "\n",
    "                                              # Append the valid query to the clip's annotations\n",
    "                                              clip[\"annotations\"][0][\"language_queries\"].append(data[\"language_queries\"][query_index])\n",
    "\n",
    "                                              # Increment the counter for processed queries\n",
    "                                              num += 1\n",
    "\n",
    "                                              # Stop processing if the limit of 9000 queries is reached\n",
    "                                              if num > 9000:\n",
    "                                                  break\n",
    "\n",
    "                                          # Handle any exceptions that occur during query processing\n",
    "                                          except:\n",
    "                                              pass\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "with open(\"/content/new_nlq_train_1.json\", \"w\") as outfile:\n",
    "        json.dump(new_nlq_train, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1736437119332,
     "user": {
      "displayName": "Meriç Uluçay",
      "userId": "04294619280493082833"
     },
     "user_tz": -60
    },
    "id": "Md3hxBnUHbNy",
    "outputId": "534c8183-61cb-46f0-ec8d-57768d83633e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2977"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQ7SfUOYHbiN"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "tar_file_path = \"/content/run_2_output.tar.gz\"\n",
    "\n",
    "extract_to = \"/content/new_queries_2\"\n",
    "\n",
    "with tarfile.open(tar_file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAWi-pRuHj2X"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/new_nlq_train_1.json\", \"r\") as file:\n",
    "    new_train = json.load(file)\n",
    "\n",
    "# Path to the directory containing input JSON files\n",
    "input_dir = \"/content/new_queries_2/run_2_output\"\n",
    "\n",
    "# Initialize the new dataset using the existing new training data\n",
    "new_nlq_train= new_train\n",
    "\n",
    "# Iterate through all files in the input directory, sorted alphabetically\n",
    "for file_name in sorted(os.listdir(input_dir)):\n",
    "    file_path = os.path.join(input_dir, file_name)  # Get the full path of the file\n",
    "\n",
    "    # Ensure the path corresponds to a file\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            try:\n",
    "                data = json.load(file)  # Load the JSON content from the file\n",
    "                video_uid = file_name.split(\"_\")[0]  # Extract video_uid from the file name\n",
    "                clip_uid = file_name.split(\"_\")[1]  # Extract clip_uid from the file name\n",
    "\n",
    "                # If the video is not already in the dataset\n",
    "                if video_uid not in added_videos:\n",
    "                    video = {\"video_uid\": video_uid, \"clips\": []}  # Initialize the video structure\n",
    "                    clip = {\"clip_uid\": clip_uid}  # Initialize the clip structure\n",
    "\n",
    "                    # Find the metadata for the clip in available clips\n",
    "                    for v in avalible_clips[\"videos\"]:\n",
    "                        if v[\"video_uid\"] == video_uid:  # Match video_uid\n",
    "                            for c in v[\"clips\"]:\n",
    "                                if c[\"clip_uid\"] == clip_uid:  # Match clip_uid\n",
    "                                    # Populate time-related metadata for the clip\n",
    "                                    clip[\"video_start_sec\"] = c[\"video_start_sec\"]\n",
    "                                    clip[\"video_end_sec\"] = c[\"video_end_sec\"]\n",
    "                                    clip[\"clip_start_sec\"] = 0\n",
    "                                    clip[\"clip_end_sec\"] = c[\"video_end_sec\"] - c[\"video_start_sec\"]\n",
    "\n",
    "                    annotations = [data]  # Initialize annotations for the clip\n",
    "                    annotations[0][\"annotation_uid\"] = \"ca7e11a2-0000-0000-0000-ea810ab6a99b\"  # Set a unique annotation UID\n",
    "\n",
    "                    # Process each query in the annotations\n",
    "                    for query_index in range(len(annotations[0][\"language_queries\"])):\n",
    "                        try:\n",
    "                            # Extract the query text from the current annotation\n",
    "                            query = annotations[0][\"language_queries\"][query_index][\"query\"]\n",
    "\n",
    "                            # Replace unwanted patterns in the query text\n",
    "                            if \" C C \" in query or \" #C C \" in query:\n",
    "                                annotations[0][\"language_queries\"][query_index][\"query\"] = query.replace(\" C C \", \" C \").replace(\" #C C \", \" C \")\n",
    "\n",
    "                            # Calculate the clip-relative start and end times for the query\n",
    "                            annotations[0][\"language_queries\"][query_index][\"clip_start_sec\"] = (annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] - clip[\"video_start_sec\"])\n",
    "                            annotations[0][\"language_queries\"][query_index][\"clip_end_sec\"] = (annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - clip[\"video_start_sec\"])\n",
    "\n",
    "                            # Filter out invalid queries based on start and end times\n",
    "                            if annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] < annotations[0][\"language_queries\"][query_index][\"video_start_sec\"]:\n",
    "                                # Remove queries where the end time is earlier than the start time\n",
    "                                annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                num -= 1\n",
    "\n",
    "                            elif annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < 1:\n",
    "                                # Adjust queries shorter than 1 second\n",
    "                                annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] += 1\n",
    "                                annotations[0][\"language_queries\"][query_index][\"clip_end_sec\"] += 1\n",
    "\n",
    "                                # Check if the adjusted query still falls outside the clip boundaries\n",
    "                                if (\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < clip[\"video_start_sec\"] or\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] > clip[\"video_end_sec\"]\n",
    "                                ):\n",
    "                                    annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                    num -= 1\n",
    "\n",
    "                            elif annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] > 8:\n",
    "                                # Remove queries longer than 8 seconds\n",
    "                                annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                num -= 1\n",
    "\n",
    "                            else:\n",
    "                                # Final check: Ensure the query fits within the clip boundaries\n",
    "                                if (\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < clip[\"video_start_sec\"] or\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] > clip[\"video_end_sec\"]\n",
    "                                ):\n",
    "                                    annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                    num -= 1\n",
    "                        except:\n",
    "                            # Remove problematic queries and decrement counter\n",
    "                            annotations[0][\"language_queries\"].pop(query_index)\n",
    "                            num -= 1\n",
    "\n",
    "                    clip[\"annotations\"] = annotations  # Add annotations to the clip\n",
    "                    num += 10  # Increment the query counter\n",
    "                    video[\"clips\"].append(clip)  # Add the clip to the video structure\n",
    "                    new_nlq_train[\"videos\"].append(video)  # Add the video to the dataset\n",
    "                    added_videos.append(video_uid)  # Mark the video as processed\n",
    "                    added_clips.append(clip_uid)  # Mark the clip as processed\n",
    "\n",
    "                    # Stop processing if limit exceeded\n",
    "                    if num > 9000:\n",
    "                        break\n",
    "                else:\n",
    "                    if clip_uid not in added_clips:\n",
    "                    #if the video is included and clip is not included\n",
    "                        clip = {\"clip_uid\": clip_uid}  # Initialize the clip structure\n",
    "\n",
    "                        # Find the metadata for the clip in available clips\n",
    "                        for v in avalible_clips[\"videos\"]:\n",
    "                            if v[\"video_uid\"] == video_uid:  # Match video_uid\n",
    "                                for c in v[\"clips\"]:\n",
    "                                    if c[\"clip_uid\"] == clip_uid:  # Match clip_uid\n",
    "                                        # Populate time-related metadata for the clip\n",
    "                                        clip[\"video_start_sec\"] = c[\"video_start_sec\"]\n",
    "                                        clip[\"video_end_sec\"] = c[\"video_end_sec\"]\n",
    "                                        clip[\"clip_start_sec\"] = 0\n",
    "                                        clip[\"clip_end_sec\"] = c[\"video_end_sec\"] - c[\"video_start_sec\"]\n",
    "\n",
    "                        annotations = [data]  # Initialize annotations for the clip\n",
    "                        annotations[0][\"annotation_uid\"] = \"ca7e11a2-0000-0000-0000-ea810ab6a99b\"  # Set a unique annotation UID\n",
    "\n",
    "                        # Process each query in the annotations\n",
    "                        for query_index in range(len(annotations[0][\"language_queries\"])):\n",
    "                            try:\n",
    "                                # Extract the query text from the current annotation\n",
    "                                query = annotations[0][\"language_queries\"][query_index][\"query\"]\n",
    "\n",
    "                                # Replace unwanted patterns in the query text\n",
    "                                if \" C C \" in query or \" #C C \" in query:\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"query\"] = query.replace(\" C C \", \" C \").replace(\" #C C \", \" C \")\n",
    "\n",
    "                                # Calculate the clip-relative start and end times for the query\n",
    "                                annotations[0][\"language_queries\"][query_index][\"clip_start_sec\"] = (annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] - clip[\"video_start_sec\"])\n",
    "                                annotations[0][\"language_queries\"][query_index][\"clip_end_sec\"] = (annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - clip[\"video_start_sec\"])\n",
    "\n",
    "                                # Filter out invalid queries based on start and end times\n",
    "                                if annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] < annotations[0][\"language_queries\"][query_index][\"video_start_sec\"]:\n",
    "                                    # Remove queries where the end time is earlier than the start time\n",
    "                                    annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                    num -= 1\n",
    "\n",
    "                                elif annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < 1:\n",
    "                                    # Adjust queries shorter than 1 second\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] += 1\n",
    "                                    annotations[0][\"language_queries\"][query_index][\"clip_end_sec\"] += 1\n",
    "\n",
    "                                    # Check if the adjusted query still falls outside the clip boundaries\n",
    "                                    if (\n",
    "                                        annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < clip[\"video_start_sec\"] or\n",
    "                                        annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] > clip[\"video_end_sec\"]\n",
    "                                    ):\n",
    "                                        annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                        num -= 1\n",
    "\n",
    "                                elif annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] > 8:\n",
    "                                    # Remove queries longer than 8 seconds\n",
    "                                    annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                    num -= 1\n",
    "\n",
    "                                else:\n",
    "                                    # Final check: Ensure the query fits within the clip boundaries\n",
    "                                    if (\n",
    "                                        annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] < clip[\"video_start_sec\"] or\n",
    "                                        annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] > clip[\"video_end_sec\"]\n",
    "                                    ):\n",
    "                                        annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                        num -= 1\n",
    "                            except:\n",
    "                                # Remove problematic queries and decrement counter\n",
    "                                annotations[0][\"language_queries\"].pop(query_index)\n",
    "                                num -= 1\n",
    "                        clip[\"annotations\"]=annotations # Add annotations to the clip\n",
    "                        for video in new_nlq_train[\"videos\"]:\n",
    "                            #print(video)\n",
    "                            if video[\"video_uid\"]==video_uid:\n",
    "                                video[\"clips\"].append(clip) # Add the clip to the video structure\n",
    "                        added_clips.append(clip_uid)  # Mark the clip as processed\n",
    "                        num+=10 # Increment the query counter\n",
    "                        # Stop processing if limit exceeded\n",
    "                        if num>9000:\n",
    "                            break\n",
    "                    else:\n",
    "                      ##if the video is included and clip is included\n",
    "                      #print(\"hi\")\n",
    "                      # Iterate through all videos in the dataset\n",
    "                      for video in new_nlq_train[\"videos\"]:\n",
    "                          # Check if the current video matches the video_uid being processed\n",
    "                          if video[\"video_uid\"] == video_uid:\n",
    "                              # Iterate through all clips in the matching video\n",
    "                              for clip in video[\"clips\"]:\n",
    "                                  # Check if the current clip matches the clip_uid being processed\n",
    "                                  if clip[\"clip_uid\"] == clip_uid:\n",
    "                                      # Iterate through all queries in the input data\n",
    "                                      for query_index in range(len(data[\"language_queries\"])):\n",
    "                                          try:\n",
    "                                              # Extract the query text for the current annotation\n",
    "                                              query = data[\"language_queries\"][query_index][\"query\"]\n",
    "\n",
    "                                              # Replace unwanted patterns in the query text\n",
    "                                              if \" C C \" in query or \" #C C \" in query:\n",
    "                                                  data[\"language_queries\"][query_index][\"query\"] = query.replace(\" C C \", \" C \").replace(\" #C C \", \" C \")\n",
    "\n",
    "                                              # Calculate clip-relative start and end times for the query\n",
    "                                              data[\"language_queries\"][query_index][\"clip_start_sec\"] = (data[\"language_queries\"][query_index][\"video_start_sec\"] - clip[\"video_start_sec\"])\n",
    "                                              data[\"language_queries\"][query_index][\"clip_end_sec\"] = (data[\"language_queries\"][query_index][\"video_end_sec\"] - clip[\"video_start_sec\"])\n",
    "\n",
    "                                              # Validate query timing: Skip if the end time is earlier than the start time\n",
    "                                              if data[\"language_queries\"][query_index][\"video_end_sec\"] < data[\"language_queries\"][query_index][\"video_start_sec\"]:\n",
    "                                                  continue\n",
    "\n",
    "                                              # Adjust queries shorter than 1 second\n",
    "                                              if data[\"language_queries\"][query_index][\"video_end_sec\"] - data[\"language_queries\"][query_index][\"video_start_sec\"] < 1:\n",
    "                                                  data[\"language_queries\"][query_index][\"video_end_sec\"] += 1  # Extend the query end time by 1 second\n",
    "                                                  data[\"language_queries\"][query_index][\"clip_end_sec\"] += 1  # Update the clip-relative end time accordingly\n",
    "\n",
    "                                              # Skip queries longer than 8 seconds\n",
    "                                              if annotations[0][\"language_queries\"][query_index][\"video_end_sec\"] - annotations[0][\"language_queries\"][query_index][\"video_start_sec\"] > 8:\n",
    "                                                  continue\n",
    "\n",
    "                                              # Ensure the query's start and end times fit within the clip's boundaries\n",
    "                                              if data[\"language_queries\"][query_index][\"video_start_sec\"] < clip[\"video_start_sec\"]:\n",
    "                                                  continue\n",
    "                                              if data[\"language_queries\"][query_index][\"video_end_sec\"] > clip[\"video_end_sec\"]:\n",
    "                                                  continue\n",
    "\n",
    "                                              # Append the valid query to the clip's annotations\n",
    "                                              clip[\"annotations\"][0][\"language_queries\"].append(data[\"language_queries\"][query_index])\n",
    "\n",
    "                                              # Increment the counter for processed queries\n",
    "                                              num += 1\n",
    "\n",
    "                                              # Stop processing if the limit of 9000 queries is reached\n",
    "                                              if num > 9000:\n",
    "                                                  break\n",
    "\n",
    "                                          # Handle any exceptions that occur during query processing\n",
    "                                          except:\n",
    "                                              pass\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "with open(\"/content/new_nlq_train_2.json\", \"w\") as outfile:\n",
    "        json.dump(new_nlq_train, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjrAZpXzY4t5bi2SFO2jri",
   "provenance": [
    {
     "file_id": "1Rii7pOWxuANqm8OovtYpmKirYS3rHNfS",
     "timestamp": 1736437680746
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
